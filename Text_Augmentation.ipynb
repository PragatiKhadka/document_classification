{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "\n",
    "import re\n",
    "import string \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"  \", sep=\";\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_cleaner(text):\n",
    "    # remove numbers \n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    # lower case \n",
    "    #text = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
    "    # remove any spaces\n",
    "    text = text.strip()\n",
    "    # remove any white spaces from beginning of string\n",
    "    text = text.lstrip() \n",
    "    # remove any white spaces from ending of string\n",
    "    text = text.rstrip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #removing : \\ characters  from the text\n",
    "    text = re.sub(r'(:\\S+) | (\\\\S+)', r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: texts_cleaner(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and reset index\n",
    "df = df[['text', 'label', 'translated']]\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Textual labels to numeric using Label Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map Textual labels to numeric using Label Encoder:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df[\"label2\"] = LabelEncoder().fit_transform(df[\"label\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the text augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented data \n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#Augment French by BERT\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.2) #aug_p: Percentage of word will be augmented\n",
    "\n",
    "def augment_text(df,samples,label):\n",
    "    new_text=[]\n",
    "    label2 = []\n",
    "    res = {}\n",
    "    label = label      \n",
    "\n",
    "    for ii in label:\n",
    "        df_n=df[df.label2==ii].reset_index(drop=True)\n",
    "    \n",
    "        ## data augmentation loop\n",
    "        for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
    "        \n",
    "            text = df_n.iloc[i]['text']\n",
    "            label = df_n.iloc[i]['label2']\n",
    "            label2.append(label)\n",
    "            augmented_text = aug.augment(text)\n",
    "            new_text.append(augmented_text)\n",
    "\n",
    "        res = {new_text[i]: label2[i] for i in range(len(new_text))}\n",
    "  \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes with 5 texts\n",
    "samples= 6\n",
    "num_text = 5\n",
    "x =df.groupby('label2').count().reset_index()\n",
    "df1 =x[x.text==num_text]\n",
    "label = df1['label2'].tolist()\n",
    "# Call the function\n",
    "aug_text1 = augment_text(df, samples, label)\n",
    "#aug_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes with 6 texts\n",
    "samples= 6\n",
    "num_text = 6\n",
    "df1 =x[x.text==num_text]\n",
    "label = df1['label2'].tolist()\n",
    "# Call the function: df and number of samples per class_label\n",
    "aug_text2 = augment_text(df, samples, label)\n",
    "#aug_text2\n",
    "#Updated dictionary\n",
    "aug_text1.update(aug_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes with 7 texts\n",
    "samples= 3\n",
    "num_text = 7\n",
    "df1 =x[x.text==num_text]\n",
    "label = df1['label2'].tolist()\n",
    "# Call the function: df and number of samples per class_label\n",
    "aug_text3 = augment_text(df, samples, label)\n",
    "#aug_text3\n",
    "#Updated dictionary\n",
    "aug_text1.update(aug_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes with 10 texts\n",
    "samples= 4\n",
    "num_text = 10\n",
    "df1 =x[x.text==num_text]\n",
    "label = df1['label2'].tolist()\n",
    "# Call the function: df and number of samples per class_label\n",
    "aug_text4 = augment_text(df, samples, label)\n",
    "#aug_text4\n",
    "#Updated dictionary\n",
    "aug_text1.update(aug_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes with 9 texts\n",
    "samples= 5\n",
    "num_text = 9\n",
    "df1 =x[x.text==num_text]\n",
    "label = df1['label2'].tolist()\n",
    "# Call the function: df and number of samples per class_label\n",
    "aug_text5 = augment_text(df, samples, label)\n",
    "aug_text5\n",
    "#Updated dictionary\n",
    "aug_text1.update(aug_text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dictionary into a dataframe\n",
    "new = pd.DataFrame(aug_text1.items(), columns=['text', 'label2'])\n",
    "df_augmented=shuffle(new).reset_index(drop=True)\n",
    "df_augmented.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add label 5 to identify that it is a augmented text by NLPaug library\n",
    "df_augmented['translated'] = 5\n",
    "#Drop duplicates\n",
    "df_augmented.drop_duplicates(inplace=True)\n",
    "df_augmented.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering the columns\n",
    "df = df[['text','label2','translated']]\n",
    "#Append DataFrames\n",
    "df_final = df.append(df_augmented, ignore_index=True)\n",
    "df_final.drop_duplicates(inplace=True)\n",
    "df_final=shuffle(df_final).reset_index(drop=True)\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('augmented_text_byNLPaug.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myNLPaug')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20acd309b8488ca3010e43d9bbb80fa92cb5520871cecfe2aaa4969c71f4d6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
